{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw5.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 5: Putting it all together \n",
    "### Associated lectures: All material till lecture 13 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "    \n",
    "## Instructions  \n",
    "rubric={points}\n",
    "\n",
    "You will earn points for following these instructions and successfully submitting your work on Gradescope.  \n",
    "\n",
    "### Before you start  \n",
    "\n",
    "- Please **read carefully\n",
    "[Use of Generative AI policy](https://ubc-cs.github.io/cpsc330-2025W1/syllabus.html#use-of-generative-ai-in-the-course)** before starting the homework assignment. \n",
    "  \n",
    "- Review the [CPSC 330 homework instructions](https://ubc-cs.github.io/cpsc330-2025W1/docs/homework_instructions.html) for detailed guidance on completing and submitting assignments. \n",
    "\n",
    "### Group work instructions\n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2.\n",
    "  \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "- If you would like to use late tokens for the homework, all group members must have the necessary late tokens available. Please note that the late tokens will be counted for all members of the group.   \n",
    "### Before submitting  \n",
    "\n",
    "- **Run all cells** (▶▶ button) to ensure the notebook executes cleanly from top to bottom.\n",
    "\n",
    "  - Execution counts must start at **1** and be sequential.\n",
    "    \n",
    "  - Notebooks with missing outputs or errors may lose marks.\n",
    "\n",
    "- **Do not upload or push data files** used in this lab to GitHub or Gradescope. (A `.gitignore` is provided to prevent this.)  \n",
    "\n",
    "\n",
    "### Submitting on Gradescope  \n",
    "\n",
    "- Upload **only** your `.ipynb` file (with outputs shown) and any required output files. Do **not** submit extra files.\n",
    "  \n",
    "- If needed, refer to the [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/).  \n",
    "- If your notebook is too large to render, also upload a **Web PDF** or **HTML** version.  \n",
    "  - You can create one using **File $\\rightarrow$ Save and Export Notebook As**.  \n",
    "  - If you get an error when creating a PDF, try running the following commands in your lab directory:  \n",
    "\n",
    "    ```bash\n",
    "    conda install -c conda-forge nbconvert-playwright\n",
    "    jupyter nbconvert --to webpdf lab1.ipynb\n",
    "    ```  \n",
    "\n",
    "  - Ensure all outputs are visible in your PDF or HTML file; TAs cannot grade your work if outputs are missing.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Unlike previous assignments, this one is open-ended and project-style. Treat it as an opportunity to explore, experiment, and learn._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Imports\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV, \n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this homework you will be working on an open-ended mini-project, where you will put all the different things you have learned so far together to solve an interesting problem.\n",
    "\n",
    "A few notes and tips when you work on this mini-project: \n",
    "\n",
    "#### Tips\n",
    "1. This mini-project is open-ended, and while working on it, there might be some situations where you'll have to use your own judgment and make your own decisions (as you would be doing when you work as a data scientist). Make sure you explain your decisions whenever necessary. \n",
    "2. **Do not include everything you ever tried in your submission** -- it's fine just to have your final code. That said, your code should be reproducible and well-documented. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code. \n",
    "3. If you realize that you are repeating a lot of code try to organize it in functions. Clear presentation of your code, experiments, and results is the key to be successful in this lab. You may use code from lecture notes or previous lab solutions with appropriate attributions. \n",
    "\n",
    "#### Assessment\n",
    "We plan to grade fairly and leniently. We don't have some secret target score that you need to achieve to get a good grade. **You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results.** For example, if you just have a bunch of code and no text or figures, that's not good. If you do a bunch of sane things and get a lower accuracy than your friend, don't sweat it.\n",
    "\n",
    "\n",
    "#### A final note\n",
    "Finally, this style of this \"project\" question is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"a few hours\" (10-14 hours???) is a good guideline for this project . Of course if you're having fun you're welcome to spend as much time as you want! But, if so, try not to do it out of perfectionism or getting the best possible grade. Do it because you're learning and enjoying it. Students from the past cohorts have found such kind of labs useful and fun and I hope you enjoy it as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1. Pick your problem and explain the prediction problem\n",
    "<hr>\n",
    "rubric={points:3}\n",
    "\n",
    "In this mini project, you have the option to choose on which dataset you will be working on. The tasks you will need to carry on will be similar, independently of your choice.\n",
    "\n",
    "### Option 1\n",
    "You can choose to work on a classification problem of predicting whether a credit card client will default or not. \n",
    "For this problem, you will use [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). In this data set, there are 30,000 examples and 24 features, and the goal is to estimate whether a person will default (fail to pay) their credit card bills; this column is labeled \"default.payment.next.month\" in the data. The rest of the columns can be used as features. You may take some ideas and compare your results with [the associated research paper](https://www.sciencedirect.com/science/article/pii/S0957417407006719), which is available through [the UBC library](https://www.library.ubc.ca/). \n",
    "\n",
    "\n",
    "### Option 2\n",
    "You can choose to work on a regression problem using a [dataset](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) of New York City Airbnb listings from 2019. As usual, you'll need to start by downloading the dataset, then you will try to predict `reviews_per_month`, as a proxy for the popularity of the listing. Airbnb could use this sort of model to predict how popular future listings might be before they are posted, perhaps to help guide hosts create more appealing listings. In reality they might instead use something like vacancy rate or average rating as their target, but we do not have that available here.\n",
    "\n",
    "> Note there is an updated version of this dataset with more features available [here](http://insideairbnb.com/). The features were are using in `listings.csv.gz` for the New York city datasets. You will also see some other files like `reviews.csv.gz`. For your own interest you may want to explore the expanded dataset and try your analysis there. However, please submit your results on the dataset obtained from Kaggle.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. Review the available datasets and choose the one you find most interesting. It may help to read through the dataset documentation on Kaggle before deciding.\n",
    "2. Once you've selected a dataset, take time to understand the problem it represents and the meaning of each feature. Use the Kaggle documentation to guide you.\n",
    "3. Download the dataset and load it into a pandas DataFrame.\n",
    "4. Write a few sentences summarizing your initial thoughts about the problem and the dataset.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "The problem is a classification problem with two possible target classes, either default or not default. From the Kaggle documentation, it appears that all of the features are numerical and thus most models could be applied to the dataset. Although we might need to potentially impute any missing values and will need to scale as there are likely order of magnitude differences between the features. Overall, the features provided seem to be relevant to our goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "credit_df = pd.read_csv(\"data/UCI_Credit_Card.csv\")\n",
    "credit_df = credit_df.set_index(\"ID\")\n",
    "credit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2. Data splitting\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train (70%) and test (30%) portions with `random_state=123`.\n",
    "\n",
    "> If your computer cannot handle training on 70% training data, make the test split bigger.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "X_train, X_test= train_test_split(credit_df, test_size=0.3, random_state=123)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3. EDA\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. **Perform exploratory data analysis (EDA)**: Conduct an initial exploration of the training set to better understand its characteristics.\n",
    "\n",
    "2. **Summarize and visualize the data**: Include at least **two summary statistics** and **two visualizations** that you find informative. For each, write **one sentence** explaining what insight it provides.\n",
    "\n",
    "3. **Record your observations**: Summarize your **initial observations** about the dataset based on your EDA.\n",
    "\n",
    "4. **Select evaluation metrics**: Choose one or more **appropriate metrics** for assessing model performance and briefly justify your choice.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if there are any missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "The dataset has no missing values. Now, looking closer at the features we can observe the following:\n",
    "\n",
    "- The education features has some issues. It can be seen as ordinal, but currently it is ordered in the opposite direction with higher numbers given to lower education (graduate school = 1, university = 2, high school = 3). It also has some duplicate value issue with unknown = 5 and unknown = 6. Both of these need to be dealt with\n",
    "- PAY_0 (payment status in september which is the current month) and months closer to now, is likely more important in predicting the default status next month. A person who has not paid for a couple of months already is more likely to default again. This feature importance could potentially be explored using decision trees.\n",
    "- The relationship between PAY_AMNTK and BILL_AMNTK could be useful. Logically, the clients that default are likely to only have been a paying a small percentage of their bill statements in the last couple of months\n",
    "- The trend of PAY_K and BILL_AMNTK (the payment status in a specific month and the bill statement in a specific month) could logically be a strong predictor of defaulting. Someone who is not paying already and has an increasing bill statement for the last couple of months is likely to not pay next month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate the relationship between PAY_AMNTK and BILL_AMNTK, we can measure the average percentage of bill statement payments (average bill to payment ratio) made by the defaulters and non-defaulters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_cols = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n",
    "pay_cols  = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "\n",
    "credit_df['pay_ratio'] = (credit_df[pay_cols].sum(axis=1) / (credit_df[bill_cols].sum(axis=1) + 1e-6)) # small addition to avoid 0 division\n",
    "\n",
    "# We need to filter out the extreme outliers where a person with very low statment ends up overpaying by a lot\n",
    "# otherwise this payment would dominate the mean\n",
    "credit_df_filtered = credit_df[(credit_df['pay_ratio'] > 0) & (credit_df['pay_ratio'] < 5)]\n",
    "\n",
    "credit_df_filtered.groupby('default.payment.next.month')['pay_ratio'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this statistic, we can see that a client who defaults pays, on average, 12 percent less per month towards their bill statement than a client who does not default. To get a clearer picture, we can visualize the payment to bill ratio and default status using a histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_ratio_non_default = credit_df_filtered[credit_df_filtered['default.payment.next.month']==0]['pay_ratio']\n",
    "pay_ratio_default = credit_df_filtered[credit_df_filtered['default.payment.next.month']==1]['pay_ratio']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(pay_ratio_non_default, bins=50, alpha=0.6, color='green', label='Non-Defaulters')\n",
    "plt.hist(pay_ratio_default, bins=50, alpha=0.6, color='red', label='Defaulters')\n",
    "\n",
    "plt.xlabel('Average Payment-to-Bill Ratio')\n",
    "plt.ylabel('Number of Clients')\n",
    "plt.title('Distribution of Average Payment-to-Bill Ratio by Default Status')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows that most clients have average payment to bill ratios aroud 0, but the ratios greater than that are usually clients who do not default. Although there is class imbalance, the histogram shows that clients who default cluster near 0 payment to bill ratio. This suggests that the average payment to bill statement ratio could be a valuable feature for our goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate the trend of PAY_K and AMNT_K, we can calculate the average monthly change in bill amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_cols = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n",
    "credit_df['bill_trend'] = credit_df[bill_cols].iloc[:, ::-1].diff(axis=1).mean(axis=1)\n",
    "credit_df.groupby('default.payment.next.month')['bill_trend'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that on average, the people who default next month have lower increase in bill statement by 500 NT dollars in the last couple of months than those who do not default. This leads to the question that perhaps those with a higher credit limit (who ar able to add more to the bill statement each month) are less likely to default. This would make sense as higher credit is usually given to those with higher salaries and a long record of always paying on time. We can investigate this by visualizing the relationship between credit limit and default status. We will do this using a histogram which will show us the distribution of the credit limit and the default status using color coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "defaulted = credit_df[credit_df['default.payment.next.month'] == 1]['LIMIT_BAL']\n",
    "non_defaulted = credit_df[credit_df['default.payment.next.month'] == 0]['LIMIT_BAL']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(non_defaulted, bins=50, alpha=0.6, color='green', label='No Default')\n",
    "plt.hist(defaulted, bins=50, alpha=0.6, color='red', label='Default')\n",
    "\n",
    "plt.title('Distribution of Credit Limit by Default Status')\n",
    "plt.xlabel('Credit Limit (NT dollars)')\n",
    "plt.ylabel('Number of Clients')\n",
    "plt.legend(title='Default Next Month')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows a clear trend, that the clients who default next month tend to have lower credit limits than those who do not default. Thus, the credit limit could be an important feature in predicting whether a client defaults or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide which evaluation metrics to use, we must check for potential class imbalance in the dataset. We will do this on the entire dataset to get a clear idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class_counts = credit_df.groupby(\"default.payment.next.month\").size()\n",
    "class_counts\n",
    "\n",
    "class_prop = credit_df.groupby(\"default.payment.next.month\").size() / len(credit_df) * 100\n",
    "class_prop\n",
    "\n",
    "class_imbalance = pd.DataFrame({\"counts\" : class_counts, \"proportion\" : class_prop})\n",
    "class_imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear class imbalance in this dataset and we cannot use accuracy as our metric. To better judge our models, given the imbalance, we must use precision, recall, and f1 score as our metrics. Since our goal is to predict whether a person will default, and as there are less cases of a person defaulting, we will take the class where the person does default as the \"positive\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 4. Feature engineering\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Perform feature engineering**: Create new features that are relevant to the problem and use this updated feature set in the following exercises. You may need to iterate between **feature engineering** and **preprocessing** to refine your features and improve model performance.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the EDA section, one useful feature that can be added is the average payment to bill ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 5. Preprocessing and transformations\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Identify feature types**: Determine the different types of features in your dataset (e.g., numerical, categorical, ordinal, text) and specify the transformations you would apply to each type.\n",
    "\n",
    "2. **Define a column transformer (if needed)**: Implement a `ColumnTransformer` to apply the appropriate preprocessing steps to each feature type.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no missing values in the dataset, there is no need for imputation on any features. \n",
    "\n",
    "We have the following ordinal features: education and PAY_* (all of the repayment status columns). We need to fix the ordering issues of the education column, by switching the order so higher education has a higher value (i.e Graduate School = 3, University = 2...) and also remove the duplicate value for \"unkown\". The same does not need to happen to PAY_* as it has meaning in its order, with higher numbers given to more months of missed payments. We don't need to scale these features, as once the other features that need to be scaled are scaled, the order of magnitude difference will not be large enough to have serious effects (these features already have values close to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# First we need to fix the education feature\n",
    "education_map = {\n",
    "    0: 0,   # invalid 0s in dataset\n",
    "    1: 3,   # graduate school = 3\n",
    "    2: 2,   # university = 2\n",
    "    3: 1,   # high school = 1\n",
    "    4: 0,   # others = 0\n",
    "    5: 0,  # unknown will be lumped with others (less noise)\n",
    "    6: 0   \n",
    "}\n",
    "\n",
    "X_train['EDUCATION'] = credit_df['EDUCATION'].map(education_map).astype(int)\n",
    "\n",
    "# This transformation is just a rule and doesn't learn anything from the test set\n",
    "X_test['EDUCATION'] = credit_df['EDUCATION'].map(education_map).astype(int)\n",
    "\n",
    "# Now we can outline the ordinal features\n",
    "Ordinal_features = ['EDUCATION', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features are already ordinal encoded so we won't need to apply the ordinal encoder transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following categorical features: marriage and sex. Both will require one-hot encoding as they have no ordinality. Although, since sex is a binary feature for this dataset, we will treat it as binary and have one binary column for it (to avoid mirror columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Binary_features = ['SEX']\n",
    "binary_transformer = OneHotEncoder(drop=\"if_binary\", dtype=int)\n",
    "\n",
    "\n",
    "Categorical_features = ['MARRIAGE']\n",
    "categorical_transformer = OneHotEncoder(sparse_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following numeric features that all need to be scaled: LIMIT_BAL, AGE, BILL_AMNT*, PAY_AMNT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numeric_features = ['LIMIT_BAL', 'AGE', 'BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6', \n",
    "                    'PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "\n",
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now need to create a column transformer to apply the necessary transformation to the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer((numeric_transformer, Numeric_features),\n",
    "                                       (binary_transformer, Binary_features),\n",
    "                                       (categorical_transformer, Categorical_features),\n",
    "                                       verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 6. Baseline model\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Establish a baseline**: Use one of `scikit-learn`’s baseline models (e.g., `DummyClassifier` or `DummyRegressor`, depending on your task) and report the results. This will serve as a reference point for evaluating the performance of your more advanced models.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_6\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 7. Linear models \n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Train a linear model**: Use a linear model as your first real attempt at solving the problem.\n",
    "\n",
    "2. **Tune hyperparameters**: Perform hyperparameter tuning to explore different values of the model's complexity parameter. \n",
    "\n",
    "3. **Evaluate with cross-validation**: Report the cross-validation scores along with their standard deviation.\n",
    "\n",
    "4. **Summarize findings**: Summarize your results, highlighting key observations from your experiments.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_7\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 8. Different models\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Experiment with additional models**: Train at least **three models** other than a linear model. Ensure that **at least one** of these models is a **tree-based ensemble model** (e.g., Random Forest, Gradient Boosting, or XGBoost).\n",
    "\n",
    "2. **Compare and interpret results**: Summarize your findings in terms of **overfitting/underfitting** behavior and **fit/score times** for each model. Reflect on your results. Were you able to **outperform the linear model**?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_8\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 9. Feature selection \n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Perform feature selection**: Attempt to select relevant features using methods such as `RFECV` or forward selection.\n",
    "\n",
    "2. **Evaluate the impact** Compare the model performance before and after feature selection. Do the results improve with feature selection?\n",
    "\n",
    "3. **Summarize findings** Summarize your observations and decide whether to **keep feature selection** in your pipeline.  If it improves results, retain it for the next exercises; otherwise, you may choose to omit it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_9\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 10. Hyperparameter optimization\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Optimize hyperparameters**: Attempt to optimize hyperparameters for the models you have tried so far. In at least **one case**, tune **multiple hyperparameters** for a single model.\n",
    "\n",
    "2. **Use suitable optimization methods**: You may use any of the following approaches for hyperparameter optimization:\n",
    "   - [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)  \n",
    "   - [`RandomizedSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)  \n",
    "   - [Bayesian optimization with scikit-optimize](https://github.com/scikit-optimize/scikit-optimize)\n",
    "\n",
    "3. **Summarize your results**: Report and compare the optimized results across models. Discuss whether hyperparameter optimization led to performance improvements.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_10\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 11. Interpretation and feature importances\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Interpret model feature importance**: Use one of the interpretation methods discussed in class (e.g., `shap`), or another suitable method of your choice, to examine the most important features of one of your **non-linear models**.\n",
    "\n",
    "2. **Summarize insights**: Summarize your observations about which features contribute most to the model's predictions and how they influence the outcomes.\n",
    "\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_11\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 12. Results on the test set\n",
    "<hr>\n",
    "\n",
    "rubric={points:10}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Evaluate on the test set**: Apply your best-performing model to the test data and report the test scores.\n",
    "\n",
    "2. **Compare and reflect**: Compare the **test scores** with the **validation scores** from previous experiments. Discuss the consistency between them. How much do you **trust your results**? Reflect on whether you might have encountered **optimization bias**.\n",
    "\n",
    "3. **Explain individual predictions**: Select one or two examples from your test predictions and use an interpretation method (e.g., **SHAP force plots**) to explain these individual predictions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_12\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 13. Summary of results\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "Imagine you are preparing to present the summary of your results to your boss and co-workers.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "1. **Summarize key results**: Create a clear and concise table highlighting your most important results (e.g., models compared, validation/test scores, key observations).\n",
    "\n",
    "2. **Write concluding remarks**: Summarize your main takeaways from the project, including what worked well and what did not.\n",
    "\n",
    "3. **Propose future improvements**: Discuss ideas or approaches you did not try but that could potentially improve **performance** or **interpretability**.\n",
    "\n",
    "4. **Report final results**: Report your **final test score** and the **metric** you used.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_13\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 14. Your takeaway\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from the supervised machine learning material we have learned so far? Please write thoughtful answers.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_14\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting your assignment, please ensure you have followed all the steps in the **Instructions** section at the top.  \n",
    "\n",
    "### Submission checklist  \n",
    "\n",
    "- [ ] Restart the kernel and run all cells (▶▶ button)\n",
    "- [ ] Make sure to push the most up to date version of your homework assignment to your GitHub repository so that we can use it for grading if there are any problems with your submission on Gradescope. \n",
    "- [ ] The `.ipynb` file runs without errors and shows all outputs.  \n",
    "- [ ] Only the `.ipynb` file and required output files are uploaded (no extra files).  \n",
    "- [ ] If the `.ipynb` file is too large to render on Gradescope, upload a Web PDF and/or HTML version as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a tricky one but you did it 👏👏!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
